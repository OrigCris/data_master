{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a65e99b7-dbd9-46d3-b620-40ca4050ab1e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Dimensão: CLIENTES (Bronze)\n",
    "# Geração de dados sintéticos com Faker pt_BR e gravação como Tabela Managed em UC\n",
    "# Padrão de namespace: <catalog>.<schema>.dim_clientes\n",
    "#\n",
    "# Parâmetros esperados (dbutils.widgets):\n",
    "#   - catalog (default: prd)\n",
    "#   - schema  (default: b_dm_callcenter)\n",
    "#   - table   (default: dim_clientes)\n",
    "#   - qtd_clientes (default: 10000)\n",
    "# ============================================================\n",
    "\n",
    "from pyspark.sql import functions as F\n",
    "from datetime import datetime\n",
    "import random\n",
    "from faker import Faker\n",
    "\n",
    "# ---------------- Parâmetros ----------------\n",
    "try:\n",
    "    catalog = dbutils.widgets.get(\"catalog\")\n",
    "except:\n",
    "    dbutils.widgets.text(\"catalog\", \"prd\")\n",
    "    catalog = dbutils.widgets.get(\"catalog\")\n",
    "\n",
    "try:\n",
    "    schema = dbutils.widgets.get(\"schema\")\n",
    "except:\n",
    "    dbutils.widgets.text(\"schema\", \"b_dm_callcenter\")\n",
    "    schema = dbutils.widgets.get(\"schema\")\n",
    "\n",
    "try:\n",
    "    table = dbutils.widgets.get(\"table\")\n",
    "except:\n",
    "    dbutils.widgets.text(\"table\", \"dim_clientes\")\n",
    "    table = dbutils.widgets.get(\"table\")\n",
    "\n",
    "try:\n",
    "    qtd_clientes = int(dbutils.widgets.get(\"qtd_clientes\"))\n",
    "except:\n",
    "    dbutils.widgets.text(\"qtd_clientes\", \"1000\")\n",
    "    qtd_clientes = int(dbutils.widgets.get(\"qtd_clientes\"))\n",
    "\n",
    "fake = Faker(\"pt_BR\")\n",
    "\n",
    "segmentos = [\"Varejo\", \"Prime\", \"Private\", \"PJ\", \"Governo\"]\n",
    "ufs = ['SP', 'RJ', 'MG', 'BA', 'RS', 'PR', 'SC', 'PE', 'CE', 'DF']\n",
    "\n",
    "# ---------------- Geração dos dados ----------------\n",
    "clientes = []\n",
    "for i in range(1, qtd_clientes + 1):\n",
    "    clientes.append({\n",
    "        \"id_cliente\": i,\n",
    "        \"nome\": fake.name(),\n",
    "        \"cpf\": fake.ssn(),\n",
    "        \"email\": fake.email(),\n",
    "        \"data_nascimento\": fake.date_of_birth(minimum_age=18, maximum_age=80),\n",
    "        \"sexo\": random.choice([\"Masculino\", \"Feminino\", \"Outro\"]),\n",
    "        \"estado\": random.choice(ufs),\n",
    "        \"cidade\": fake.city(),\n",
    "        \"segmento\": random.choice(segmentos),\n",
    "        \"dt_cadastro\": datetime.now(),\n",
    "    })\n",
    "\n",
    "df = spark.createDataFrame(clientes) \\\n",
    "    .withColumn(\"dat_ref_carga\", F.current_date()) \\\n",
    "    .withColumn(\"ingestion_ts\", F.current_timestamp())\n",
    "\n",
    "db_table = f\"{catalog}.{schema}.{table}\"\n",
    "\n",
    "(\n",
    "    df.write\n",
    "      .format(\"delta\")\n",
    "      .mode(\"overwrite\")\n",
    "      .option(\"overwriteSchema\", \"true\")\n",
    "      .saveAsTable(db_table) \n",
    ")\n",
    "\n",
    "spark.sql(f\"OPTIMIZE {db_table}\")\n",
    "\n",
    "print(f\"[OK] Dimensão criada/atualizada: {db_table}\")\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "3"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "bronze_dim_clientes",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
