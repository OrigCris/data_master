{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7001ac2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "#   - **Conexão**: Event Hubs (connection string no AKV via Secret Scope)\n",
    "#   - **Persistência**: Delta *managed* em `catalog.schema.tabela`\n",
    "#   - **Clustering**: **Delta Liquid Clustering** por `ingestion_date`\n",
    "#   - **Trigger**: `once`, com **checkpoint dedicado** por tópico/tabela\n",
    "#   - **Uso**: 1 notebook parametrizado → 3 tarefas (1 por tópico)\n",
    "# ============================================================\n",
    "\n",
    "\n",
    "# ===================== PARÂMETROS (Widgets) =====================\n",
    "CATALOG           = dbutils.widgets.get(\"catalog\")\n",
    "SCHEMA            = dbutils.widgets.get(\"schema\")\n",
    "TABLE_NAME        = dbutils.widgets.get(\"table_name\")\n",
    "EVENTHUB_NAME     = dbutils.widgets.get(\"eventhub_name\")\n",
    "SECRET_SCOPE      = dbutils.widgets.get(\"secret_scope\")\n",
    "SECRET_KEY        = dbutils.widgets.get(\"secret_key\")\n",
    "CHECKPOINT_BASE   = dbutils.widgets.get(\"checkpoint_base\").rstrip(\"/\")\n",
    "\n",
    "assert TABLE_NAME,     \"Param 'table_name' é obrigatório\"\n",
    "assert EVENTHUB_NAME,  \"Param 'eventhub_name' é obrigatório\"\n",
    "assert SECRET_SCOPE,   \"Param 'secret_scope' é obrigatório\"\n",
    "assert SECRET_KEY,     \"Param 'secret_key' é obrigatório\"\n",
    "\n",
    "FQN = f\"{CATALOG}.{SCHEMA}.{TABLE_NAME}\"\n",
    "print(f\"Destino: {FQN}\")\n",
    "\n",
    "# ===================== IMPORTS & UTILS =====================\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql import types as T\n",
    "import json\n",
    "\n",
    "def _read_secret(scope: str, key: str) -> str:\n",
    "    try:\n",
    "        return dbutils.secrets.get(scope=scope, key=key)\n",
    "    except Exception as e:\n",
    "        raise RuntimeError(f\"Falha ao ler secret '{scope}:{key}'. Verifique Secret Scope/AKV. Detalhe: {e}\")\n",
    "\n",
    "def _ensure_table_with_liquid(fqn: str):\n",
    "    # cria a tabela se não existir e habilita liquid clustering por ingestion_date\n",
    "    if not spark.catalog.tableExists(fqn):\n",
    "        spark.sql(f\"\"\"\n",
    "            CREATE TABLE IF NOT EXISTS {fqn} (\n",
    "            body             STRING,\n",
    "            partition        INT,\n",
    "            offset           STRING,\n",
    "            sequenceNumber   LONG,\n",
    "            enqueuedTime     TIMESTAMP,\n",
    "            partitionKey     STRING,\n",
    "            ingestion_ts     TIMESTAMP,\n",
    "            ingestion_date   DATE\n",
    "            )\n",
    "            USING DELTA\n",
    "            CLUSTER BY (ingestion_date)\n",
    "        \"\"\")\n",
    "\n",
    "        spark.sql(f\"\"\"\n",
    "            ALTER TABLE {fqn}\n",
    "            SET TBLPROPERTIES (delta.enableChangeDataFeed = true);\n",
    "        \"\"\")\n",
    "\n",
    "# ===================== CONEXÃO: EVENT HUBS =====================\n",
    "conn_str = _read_secret(SECRET_SCOPE, SECRET_KEY)\n",
    "eh_options = {\n",
    "   'eventhubs.connectionString': sc._jvm.org.apache.spark.eventhubs.EventHubsUtils.encrypt(f\"{conn_str};EntityPath={EVENTHUB_NAME}\"),\n",
    "    'eventhubs.startingPosition': '{\"offset\": \"-1\", \"seqNo\": -1, \"enqueuedTime\": null, \"isInclusive\": true}'\n",
    "}\n",
    "\n",
    "# ===================== CRIAR TABELA CASO NÃO EXISTA =====================\n",
    "_ensure_table_with_liquid(FQN)\n",
    "\n",
    "# ===================== READ STREAM & NORMALIZAÇÃO BRONZE =====================\n",
    "raw = (spark.readStream\n",
    "       .format(\"eventhubs\")\n",
    "       .options(**eh_options)\n",
    "       .load())\n",
    "\n",
    "bronze_df = (\n",
    "    raw.select(\n",
    "        F.col(\"body\").cast(\"string\").alias(\"body\"),\n",
    "        F.col(\"partition\").cast(\"int\").alias(\"partition\"),\n",
    "        F.col(\"offset\").cast(\"string\").alias(\"offset\"),\n",
    "        F.col(\"sequenceNumber\").cast(\"long\").alias(\"sequenceNumber\"),\n",
    "        F.col(\"enqueuedTime\").cast(\"timestamp\").alias(\"enqueuedTime\"),\n",
    "        F.col(\"partitionKey\").cast(\"string\").alias(\"partitionKey\")\n",
    "    )\n",
    "    .withColumn(\"ingestion_ts\",   F.current_timestamp())\n",
    "    .withColumn(\"ingestion_date\", F.to_date(\"ingestion_ts\"))\n",
    ")\n",
    "\n",
    "# ===================== WRITE STREAM (TRIGGER ONCE) =====================\n",
    "checkpoint_path = f\"{CHECKPOINT_BASE}/{EVENTHUB_NAME}/{TABLE_NAME}\"\n",
    "\n",
    "query = (bronze_df.writeStream\n",
    "         .format(\"delta\")\n",
    "         .outputMode(\"append\")\n",
    "         .trigger(once=True)\n",
    "         .option(\"checkpointLocation\", checkpoint_path)\n",
    "         .start(FQN))\n",
    "\n",
    "query.awaitTermination()\n",
    "\n",
    "print(f\"[OK] Ingestão concluída (trigger once) → {FQN}\")\n",
    "print(f\"Checkpoint: {checkpoint_path}\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
